{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Exkaldi\n",
    "\n",
    "In this section, we will train the triphone HMM-GMM with delta feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Kaldi root directory was not found in system PATH. You can appoint it:\n",
      "exkaldi.info.reset_kaldi_root( yourPath )\n",
      "If not, ERROR will occur when implementing some core functions.\n"
     ]
    }
   ],
   "source": [
    "CONDA_DIR = \"/home/khanh/workspace/miniconda3\"\n",
    "KALDI_ENV = \"kaldi\"\n",
    "EXKALDI_ENV = \"exkaldi\"\n",
    "KALDI_ROOT = \"/home/khanh/workspace/projects/kaldi\"\n",
    "\n",
    "DATA_DIR = \"librispeech_dummy\"\n",
    "\n",
    "def import_exkaldi():\n",
    "    import os\n",
    "\n",
    "    # add lib path\n",
    "    os.environ[\"LD_LIBRARY_PATH\"] = \";\".join([\n",
    "        os.path.join(CONDA_DIR, \"envs\", KALDI_ENV, \"lib\"),\n",
    "        os.path.join(CONDA_DIR, \"envs\", EXKALDI_ENV, \"lib\"),\n",
    "    ])\n",
    "\n",
    "    import exkaldi\n",
    "    exkaldi.info.reset_kaldi_root(KALDI_ROOT)\n",
    "\n",
    "    return exkaldi\n",
    "exkaldi = import_exkaldi()\n",
    "dataDir = \"librispeech_dummy\"\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, instantiate a triphone model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.hmm.hmm.TriphoneHMM at 0x7f3948303a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = exkaldi.hmm.TriphoneHMM()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___model___ is unavaliable now. We have to initialize its data. We will use these files which have been generated in early steps.\n",
    "\n",
    "___tree___ and ___treeStats___ : generated in 6_train_decision_tree  \n",
    "___topo___:  generated in 5_train_mono_HMM-GMM\n",
    "\n",
    "Actually, you can initialize the TriphoneHMM object from feature directly, but here we use tree statistics file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"tree\")\n",
    "treeStatsFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"treeStats.acc\")\n",
    "topoFile = os.path.join(dataDir, \"exp\", \"topo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmmHmmInfo(phones=69, pdfs=264, transitionIds=684, transitionStates=342, dimension=39, gaussians=264)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.initialize(tree=treeFile, treeStatsFile=treeStatsFile, topoFile=topoFile)\n",
    "\n",
    "model.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training steps of triphone HMM are almost the same as monophone HMM except that we don't use equally aligning at the first time.\n",
    "\n",
    "We will introduce the traing step in a nutshell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in detail\n",
    "\n",
    "At the first step, we must generate the new alignment data in the first step. You can convert the lastest alignment data generated by monophone model to a new alignment data corresponding to triphone. Use the generated alignment and final monophone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliFile = os.path.join(dataDir, \"exp\", \"train_mono\", \"final.ali\")\n",
    "monoFile= os.path.join(dataDir, \"exp\", \"train_mono\", \"final.mdl\")\n",
    "ali = exkaldi.load_ali(aliFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.core.archive.BytesAliTrans at 0x7f39319c7550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newAli = exkaldi.hmm.convert_alignment(\n",
    "                                 ali=ali, \n",
    "                                 originHmm=monoFile, \n",
    "                                 targetHmm=model, \n",
    "                                 tree=treeFile\n",
    "                            )\n",
    "newAli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another way, align feature again directly with new triphone model. \n",
    "\n",
    "In the next steps, we will review the steps to train a HMM-GMM model and use it to align acoustic feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del newAli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a lexicons (generated in 3_prepare_lexicons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.decode.graph.LexiconBank at 0x7f3931a053d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexFile = os.path.join(dataDir, \"exp\", \"lexicons.lex\")\n",
    "\n",
    "lexicons = exkaldi.load_lex(lexFile)\n",
    "\n",
    "lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare int-format transcription (generated in 5_train_mono_HMM-GMM )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'103-1240-0000': '201 875 800 1004 744 653 1239 800 1004 744 725 671 1395 1268 96 751 1064 328 348 648 4 724 588 501 1416 36 53 687 367 53 1314 177 4 168'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intTransFile = os.path.join(dataDir, \"exp\", \"text.int\")\n",
    "\n",
    "trans = exkaldi.load_transcription(intTransFile)\n",
    "\n",
    "trans.subset(nHead=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare L.fst file (generated in 3_prepare_lexicons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lfile = os.path.join(dataDir, \"exp\", \"L.fst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare feature (generated in 2_feature_processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featFile = os.path.join(dataDir, \"exp\", \"train_mfcc_cmvn.ark\")\n",
    "feat = exkaldi.load_feat(featFile)\n",
    "feat = feat.add_delta(order=2)\n",
    "\n",
    "feat.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compile new train graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = os.path.join(dataDir, \"exp\", \"train_delta\")\n",
    "\n",
    "exkaldi.utils.make_dependent_dirs(outDir, pathIsFile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'librispeech_dummy/exp/train_delta/train_graph'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainGraphFile = os.path.join(outDir, \"train_graph\")\n",
    "\n",
    "model.compile_train_graph(tree=treeFile, transcription=trans, LFile=Lfile, outFile=trainGraphFile, lexicons=lexicons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Align acoustic feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.core.archive.BytesAliTrans at 0x7f3931331c40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ali = model.align(feat, trainGraphFile, lexicons=lexicons)\n",
    "\n",
    "ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Accumulate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'librispeech_dummy/exp/train_delta/stats.acc'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsFile = os.path.join(outDir, \"stats.acc\")\n",
    "\n",
    "model.accumulate_stats(feat=feat, ali=ali, outFile=statsFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Update HMM-GMM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmmHmmInfo(phones=69, pdfs=264, transitionIds=684, transitionStates=342, dimension=39, gaussians=300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetGaussians = 300\n",
    "\n",
    "model.update(statsFile, targetGaussians)\n",
    "\n",
    "model.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in high-level\n",
    "\n",
    "In this step, we will introduce how to training the triphone in directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del ali\n",
    "del trans\n",
    "\n",
    "os.remove(trainGraphFile)\n",
    "os.remove(statsFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some file paths or objects defined above will be used here. \n",
    "\n",
    "Firstly, initialize model. Give lexicons as a optional parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmmHmmInfo(phones=69, pdfs=264, transitionIds=684, transitionStates=342, dimension=39, gaussians=264)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = exkaldi.hmm.TriphoneHMM(lexicons=lexicons)\n",
    "\n",
    "model.initialize(tree=treeFile, treeStatsFile=treeStatsFile, topoFile=topoFile)\n",
    "\n",
    "model.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train triphone model.\n",
      "Start Time: 2022/09/05-15:32:16\n",
      "Convert transcription to int value format.\n",
      "Compiling training graph.\n",
      "Iter >> 1\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.0075 seconds\n",
      "Iter >> 2\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.8225 seconds\n",
      "Iter >> 3\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.5729 seconds\n",
      "Iter >> 4\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.6493 seconds\n",
      "Iter >> 5\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 3.5087 seconds\n",
      "Iter >> 6\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.6120 seconds\n",
      "Iter >> 7\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.9597 seconds\n",
      "Iter >> 8\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.6623 seconds\n",
      "Iter >> 9\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.6582 seconds\n",
      "Iter >> 10\n",
      "Aligning data\n",
      "Accumulate GMM statistics\n",
      "Update GMM parameters\n",
      "Used time: 2.8041 seconds\n",
      "Align last time with final model\n",
      "Done to train the triphone model\n",
      "Saved Final Model: librispeech_dummy/exp/train_delta/final.mdl\n",
      "Saved Ali:  librispeech_dummy/exp/train_delta/final.ali\n",
      "End Time: 2022/09/05-15:32:46\n"
     ]
    }
   ],
   "source": [
    "outDir = os.path.join(dataDir, \"exp\", \"train_delta\")\n",
    "transFile = os.path.join(dataDir, \"train\", \"text\")\n",
    "\n",
    "aliIndex = model.train(feat=feat, transcription=transFile, LFile=Lfile, tree=treeFile, tempDir=outDir, \n",
    "                         numIters=10, maxIterInc=8, totgauss=1500\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final model and alignment have been saved in file automatically. Look the final model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmmHmmInfo(phones=69, pdfs=264, transitionIds=684, transitionStates=342, dimension=39, gaussians=1652)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
