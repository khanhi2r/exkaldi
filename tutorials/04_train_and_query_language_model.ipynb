{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to ExKaldi\n",
    "\n",
    "In this section, we will train a N-Grams language model and query it.\n",
    "\n",
    "Althrough __SriLM__ is avaliable in ExKaldi, we recommend __KenLM__ toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exkaldi\n",
    "\n",
    "import os\n",
    "dataDir = \"librispeech_dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, prepare the lexicons. We have generated and saved a __LexiconBank__ object in file already (3_prepare_lexicons). So restorage it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexFile = os.path.join(dataDir, \"exp\", \"lexicons.lex\")\n",
    "\n",
    "lexicons = exkaldi.load_lex(lexFile)\n",
    "\n",
    "lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use training text corpus to train LM model. Even though we have prepared a transcription file in the data directory, we do not need the utterance-ID information at the head of each line, so we must take a bit of work to produce a new text.\n",
    "\n",
    "We can lend a hand of the exkaldi __Transcription__ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textFile = os.path.join(dataDir, \"train\", \"text\")\n",
    "\n",
    "trans = exkaldi.load_transcription(textFile)\n",
    "\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTextFile = os.path.join(dataDir, \"exp\", \"train_lm_text\")\n",
    "\n",
    "trans.save(fileName=newTextFile, discardUttID=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But actually, you don't need do this. If you use a __Transcription__ object to train the language model, the information of utterance ID will be discarded automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a 2-grams model with __KenLM__ backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpaFile = os.path.join(dataDir, \"exp\", \"2-gram.arpa\")\n",
    "\n",
    "exkaldi.lm.train_ngrams_kenlm(lexicons, order=2, text=trans, outFile=arpaFile, config={\"-S\":\"20%\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARPA model can be transform to binary format in order to accelerate loading and reduce memory cost.  \n",
    "Although __KenLM__ Python API supports reading ARPA format, but in exkaldi, we only expected KenLM Binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryLmFile = os.path.join(dataDir, \"exp\", \"2-gram.binary\")\n",
    "\n",
    "exkaldi.lm.arpa_to_binary(arpaFile, binaryLmFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the binary LM file to initialize a Python KenLM n-grams object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exkaldi.lm.KenNGrams(binaryLmFile)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KenNGrams__ is simple wrapper of KenLM python Model. Check model information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can query this model with a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score_sentence(\"HELLO WORLD\", bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a example to compute the perplexity of test corpus in order to evaluate the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalTrans = exkaldi.load_transcription( os.path.join(dataDir, \"test\", \"text\") )\n",
    "\n",
    "score = model.score(evalTrans)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___score___ is an exkaldi __Metric__ (a subclass of Python dict) object. \n",
    "\n",
    "We design a group of classes to hold Kaldi text format table and exkaldi own text format data:\n",
    "\n",
    "__ListTable__: spk2utt, utt2spk, words, phones and so on.  \n",
    "__Transcription__: transcription corpus, n-best decoding result and so on.  \n",
    "__Metric__: AM score, LM score, LM perplexity, Sentence lengthes and so on.  \n",
    "__IndexTable__: The index of binary data.  \n",
    "__WavSegment__: The wave information.  \n",
    "\n",
    "All these classes are subclasses of Python dict. They have some common and respective methods and attributes. \n",
    "\n",
    "In this case, for example, we can compute the average value of __Metric__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, the weighted average by the length os sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.mean( weight= evalTrans.sentence_length() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we use perplexity more to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.perplexity(evalTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to Language Model. If you want to use query ARPA model directly. You can use this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we use the perplexity score to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exkaldi.load_ngrams(arpaFile)\n",
    "\n",
    "model.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the termination of this section, we generate the Grammar fst for futher steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gfile = os.path.join(dataDir, \"exp\", \"G.fst\")\n",
    "\n",
    "exkaldi.decode.graph.make_G(lexicons, arpaFile, outFile=Gfile, order=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
