{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to ExKaldi\n",
    "\n",
    "In this section, we will further process the Kaldi decoding lattice and score the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Kaldi root directory was not found in system PATH. You can appoint it:\n",
      "exkaldi.info.reset_kaldi_root( yourPath )\n",
      "If not, ERROR will occur when implementing some core functions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataDir = \"librispeech_dummy\"\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/home/khanh/workspace/miniconda3/envs/kaldi/lib/;/home/khanh/workspace/miniconda3/envs/test/lib/\"\n",
    "\n",
    "import exkaldi\n",
    "exkaldi.info.reset_kaldi_root(\"/home/khanh/workspace/projects/kaldi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the lattice file (generated in 09_decode_back_HMM-GMM_and_WFST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.decode.wfst.Lattice at 0x7f27004f34f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"decode_test\", \"test.lat\")\n",
    "\n",
    "lat = exkaldi.decode.wfst.load_lat(latFile)\n",
    "\n",
    "lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be simple and straightforward, we get the 1-best result from lattice. Word-id table and HMM model are necessary.\n",
    "\n",
    "Word-ID table can be __words.txt__ file (If decoded in word level) or __phones.txt__ file (If decoded in phone level) or Exkaldi __ListTable__ object.  \n",
    "\n",
    "Ideally, __LexiconBank__ object is also avaliable because you can get both \"words\" and \"phones\" from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsFile = os.path.join(dataDir, \"exp\", \"words.txt\")\n",
    "\n",
    "hmmFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1272-128104-0000': '1376 1308 439 883 1068 1091 215 871 629 4 534 685 1023 1167 653 4 575 1391 4 1390 1409 180 585 261 38'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lat.get_1best(symbolTable=wordsFile, hmm=hmmFile, lmwt=1, acwt=0.5)\n",
    "\n",
    "result.subset(nHead=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___result___ is a exkaldi __Transcription__ object.\n",
    "\n",
    "The decoding result is int-ID format. If you want it by text-format, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1272-128104-0000': 'WAS TOOK FAULT OR ROOM SCENE CLASSED OLD IN A GOT LA RED SO IS A HER WERE A WENT WILL CAN HIS COST ALL'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textResult = exkaldi.hmm.transcription_from_int(result, wordsFile)\n",
    "\n",
    "textResult.subset(nHead=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for convenience, we restorage lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexFile = os.path.join(dataDir, \"exp\", \"lexicons.lex\")\n",
    "\n",
    "lexicons = exkaldi.load_lex(lexFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del textResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the __transcription_from_int__ function, we can transform transcription by using the __Transcription__'s own method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1272-128104-0000': 'WAS TOOK FAULT OR ROOM SCENE CLASSED OLD IN A GOT LA RED SO IS A HER WERE A WENT WILL CAN HIS COST ALL'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id = lexicons(\"words\")\n",
    "oovID = word2id[lexicons(\"oov\")]\n",
    "id2word = word2id.reverse()\n",
    "\n",
    "textResult = result.convert(symbolTable=id2word, unkSymbol=oovID)\n",
    "\n",
    "textResult.subset(nHead=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can score the decoding result. Typically, you can compute the WER(word err rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(WER=137.95, words=419, insErr=238, delErr=5, subErr=335, SER=100.0, sentences=20, wrongSentences=20, missedSentences=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refFile = os.path.join(dataDir, \"test\", \"text\")\n",
    "\n",
    "score = exkaldi.decode.score.wer(ref=refFile, hyp=textResult, mode=\"present\")\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or some times, compute the edit distance score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(editDistance=1384, words=1866, SER=1.0, sentences=20, wrongSentences=20, missedSentences=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = exkaldi.decode.score.edit_distance(ref=refFile, hyp=textResult, mode=\"present\")\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the accuracy of words levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25830653804930337"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - score.editDistance/score.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested this and only get the WER 134.37, and the accuracy rate of words is 27.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We support further process the lattice, for example, to add penalty or to scale it.\n",
    "\n",
    "Here is a example to config different language model weight(LMWT) and penalty. (In Instead of text-format result, we use int-format reference file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1272-128104-0000': '801 1000 653 1268 64 865 1268 789 216 53 1381 75 526 1304 1387 585 533'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refInt = exkaldi.hmm.transcription_to_int(refFile, lexicons(\"words\"), unkSymbol=lexicons(\"oov\"))\n",
    "refIntFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"decode_test\", \"text.int\")\n",
    "refInt.save(refIntFile)\n",
    "\n",
    "refInt.subset(nHead=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty 0.0, LMWT 10: WER 135.08\n",
      "Penalty 0.0, LMWT 11: WER 135.08\n",
      "Penalty 0.0, LMWT 12: WER 134.84\n",
      "Penalty 0.0, LMWT 13: WER 134.84\n",
      "Penalty 0.0, LMWT 14: WER 134.84\n",
      "Penalty 0.5, LMWT 10: WER 134.61\n",
      "Penalty 0.5, LMWT 11: WER 134.61\n",
      "Penalty 0.5, LMWT 12: WER 134.61\n",
      "Penalty 0.5, LMWT 13: WER 134.61\n",
      "Penalty 0.5, LMWT 14: WER 134.61\n",
      "Penalty 1.0, LMWT 10: WER 134.61\n",
      "Penalty 1.0, LMWT 11: WER 134.13\n",
      "Penalty 1.0, LMWT 12: WER 134.13\n",
      "Penalty 1.0, LMWT 13: WER 134.13\n",
      "Penalty 1.0, LMWT 14: WER 134.13\n"
     ]
    }
   ],
   "source": [
    "for penalty in [0., 0.5, 1.0]:\n",
    "    for LMWT in range(10, 15):\n",
    "        \n",
    "        newLat = lat.add_penalty(penalty)\n",
    "        result = newLat.get_1best(lexicons(\"words\"), hmmFile, lmwt=LMWT, acwt=0.5)\n",
    "\n",
    "        score = exkaldi.decode.score.wer(ref=refInt, hyp=result, mode=\"present\")\n",
    "        \n",
    "        print(f\"Penalty {penalty}, LMWT {LMWT}: WER {score.WER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lattice, you can get the phone-level result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1272-128104-0000': '<SIL> W AH0 Z T UH1 K F AO1 L T ER0 R UW1 M S IY1 N K L AE1 S T OW1 L D IH0 N AH0 G AA1 T L AA1 R EH1 D S OW1 IH0 Z <SIL> AH0 HH ER0 W ER0 AH0 W EH1 N T W AH0 L K AH0 N HH IH0 Z K AA1 S T AO1 L <SIL>'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneResult = lat.get_1best(lexicons(\"phones\"), hmmFile, lmwt=1, acwt=0.5, phoneLevel=True)\n",
    "\n",
    "phoneResult = exkaldi.hmm.transcription_from_int(phoneResult, lexicons(\"phones\"))\n",
    "\n",
    "phoneResult.subset(nHead=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From lattice, N-Best results can also be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-best <class 'exkaldi.core.archive.Transcription'>\n",
      "2-best <class 'exkaldi.core.archive.Transcription'>\n",
      "3-best <class 'exkaldi.core.archive.Transcription'>\n"
     ]
    }
   ],
   "source": [
    "result = lat.get_nbest(\n",
    "                        n=3,\n",
    "                        symbolTable=lexicons(\"words\"),\n",
    "                        hmm=hmmFile, \n",
    "                        acwt=0.5, \n",
    "                        phoneLevel=False,\n",
    "                        requireCost=False,\n",
    "                )\n",
    "\n",
    "for re in result:\n",
    "    print(re.name, type(re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___result___ is a list of N-bests __Transcription__ objects. If ___requireCost___ is True, return the LM score and AM score sumultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-best <class 'exkaldi.core.archive.Transcription'>\n",
      "2-best <class 'exkaldi.core.archive.Transcription'>\n",
      "3-best <class 'exkaldi.core.archive.Transcription'>\n",
      "AM-1-best <class 'exkaldi.core.archive.Metric'>\n",
      "AM-2-best <class 'exkaldi.core.archive.Metric'>\n",
      "AM-3-best <class 'exkaldi.core.archive.Metric'>\n",
      "LM-1-best <class 'exkaldi.core.archive.Metric'>\n",
      "LM-2-best <class 'exkaldi.core.archive.Metric'>\n",
      "LM-3-best <class 'exkaldi.core.archive.Metric'>\n"
     ]
    }
   ],
   "source": [
    "result = lat.get_nbest(\n",
    "                        n=3,\n",
    "                        symbolTable=lexicons(\"words\"),\n",
    "                        hmm=hmmFile, \n",
    "                        acwt=0.5, \n",
    "                        phoneLevel=False,\n",
    "                        requireCost=True,\n",
    "                )\n",
    "\n",
    "for re in result[0]:\n",
    "    print(re.name, type(re))\n",
    "    \n",
    "for re in result[1]:\n",
    "    print(re.name, type(re))\n",
    "\n",
    "for re in result[2]:\n",
    "    print(re.name, type(re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And importantly, Alignment can be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-best <class 'exkaldi.core.archive.NumpyAliTrans'>\n",
      "2-best <class 'exkaldi.core.archive.NumpyAliTrans'>\n",
      "3-best <class 'exkaldi.core.archive.NumpyAliTrans'>\n"
     ]
    }
   ],
   "source": [
    "result = lat.get_nbest(\n",
    "                        n=3,\n",
    "                        symbolTable=lexicons(\"words\"),\n",
    "                        hmm=hmmFile, \n",
    "                        acwt=0.5, \n",
    "                        phoneLevel=False,\n",
    "                        requireCost=False,\n",
    "                        requireAli=True,\n",
    "                )\n",
    "\n",
    "for re in result[1]:\n",
    "    print(re.name, type(re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not train __LDA+MLLT__ and __SAT__ in this tutorial. If you need tutorial about them, please look the `examples` directory. We prepare some actual recipes for, for example, __TIMIT__ corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
